{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LESSON 3B: GOAL PREDICTION MODEL**\n",
    "\n",
    "---\n",
    "\n",
    "## **What We're Building**\n",
    "\n",
    "In Lesson 3, we built a **classifier** to predict match outcomes (Win/Draw/Loss) with **51.3% accuracy**.\n",
    "\n",
    "Now we're taking a different approach:\n",
    "- Train **2 regressors** to predict `home_goals` and `away_goals`\n",
    "- **Derive** match outcomes from goal predictions (2.1 - 1.3 ‚Üí Home Win)\n",
    "- Compare accuracy to direct classification\n",
    "- Bonus: Get **score predictions** for betting markets!\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Concepts**\n",
    "\n",
    "### **Regression vs Classification**\n",
    "- **Classification**: Predict categories (Win/Draw/Loss)\n",
    "- **Regression**: Predict numbers (0, 1, 2, 3... goals)\n",
    "\n",
    "### **Evaluation Metrics**\n",
    "- **MAE** (Mean Absolute Error): Average goal difference from actual\n",
    "- **RMSE** (Root Mean Squared Error): Penalizes large errors\n",
    "- **R¬≤** (R-squared): % of variance explained (0 = baseline, 1 = perfect)\n",
    "\n",
    "---\n",
    "\n",
    "## **Dataset**\n",
    "- 1,900 matches from 2020-2025\n",
    "- 105 engineered features\n",
    "- Targets: `home_goals`, `away_goals`, `match_outcome`\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 1: SETUP & DATA LOADING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set up paths\n",
    "OUTPUT_DIR = Path('ml_project/outputs/08_goal_prediction')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR = Path('ml_project/models')\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('ml_project/data/match_features_historical.csv')\n",
    "scaler = joblib.load('ml_project/models/feature_scaler.pkl')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LESSON 3B: GOAL PREDICTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset loaded: {len(data)} matches\")\n",
    "print(f\"\\nTarget variables:\")\n",
    "print(f\"  - home_goals: mean={data['home_goals'].mean():.2f}, std={data['home_goals'].std():.2f}\")\n",
    "print(f\"  - away_goals: mean={data['away_goals'].mean():.2f}, std={data['away_goals'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 2: UNDERSTAND GOAL DISTRIBUTIONS**\n",
    "\n",
    "Before building models, let's understand what we're predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNDERSTANDING GOAL DISTRIBUTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize goal distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Home goals\n",
    "axes[0].hist(data['home_goals'], bins=range(0, 8), alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0].set_xlabel('Home Goals', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Distribution of Home Goals', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(data['home_goals'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {data[\"home_goals\"].mean():.2f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Away goals\n",
    "axes[1].hist(data['away_goals'], bins=range(0, 8), alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1].set_xlabel('Away Goals', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Distribution of Away Goals', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(data['away_goals'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {data[\"away_goals\"].mean():.2f}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'goal_distributions.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nHome Goals Stats:\")\n",
    "print(f\"  Mean: {data['home_goals'].mean():.2f}\")\n",
    "print(f\"  Median: {data['home_goals'].median():.1f}\")\n",
    "print(f\"  Std: {data['home_goals'].std():.2f}\")\n",
    "print(f\"  Range: {data['home_goals'].min():.0f} - {data['home_goals'].max():.0f}\")\n",
    "\n",
    "print(f\"\\nAway Goals Stats:\")\n",
    "print(f\"  Mean: {data['away_goals'].mean():.2f}\")\n",
    "print(f\"  Median: {data['away_goals'].median():.1f}\")\n",
    "print(f\"  Std: {data['away_goals'].std():.2f}\")\n",
    "print(f\"  Range: {data['away_goals'].min():.0f} - {data['away_goals'].max():.0f}\")\n",
    "\n",
    "print(\"\\nüí° TEACHING POINT:\")\n",
    "print(\"Home teams score MORE on average (home advantage)\")\n",
    "print(f\"Home avg: {data['home_goals'].mean():.2f} vs Away avg: {data['away_goals'].mean():.2f}\")\n",
    "print(f\"Difference: {(data['home_goals'].mean() - data['away_goals'].mean()):.2f} goals per match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 3: PREPARE DATA**\n",
    "\n",
    "Same train/validation split as Lesson 3 for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPARING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Separate features from targets\n",
    "exclude_cols = [\n",
    "    'match_id', 'season', 'date', 'gameweek',\n",
    "    'home_team', 'away_team',\n",
    "    'match_outcome', 'home_goals', 'away_goals'\n",
    "]\n",
    "\n",
    "feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "X = data[feature_cols]\n",
    "y_home_goals = data['home_goals']\n",
    "y_away_goals = data['away_goals']\n",
    "y_outcome = data['match_outcome']\n",
    "\n",
    "print(f\"‚úì Features: {X.shape[1]}\")\n",
    "print(f\"‚úì Home goals target: {len(y_home_goals)}\")\n",
    "print(f\"‚úì Away goals target: {len(y_away_goals)}\")\n",
    "\n",
    "# Train/validation split (same as Lesson 3)\n",
    "train_mask = data['season'].isin(['2020-2021', '2021-2022', '2022-2023', '2023-2024'])\n",
    "val_mask = data['season'] == '2024-2025'\n",
    "\n",
    "X_train = X[train_mask]\n",
    "X_val = X[val_mask]\n",
    "y_train_home = y_home_goals[train_mask]\n",
    "y_val_home = y_home_goals[val_mask]\n",
    "y_train_away = y_away_goals[train_mask]\n",
    "y_val_away = y_away_goals[val_mask]\n",
    "y_val_outcome = y_outcome[val_mask]\n",
    "\n",
    "print(f\"\\n‚úì Training set: {len(X_train)} matches\")\n",
    "print(f\"‚úì Validation set: {len(X_val)} matches\")\n",
    "\n",
    "# Scale features (use SAME scaler from Lesson 3)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=feature_cols, index=X_val.index)\n",
    "\n",
    "print(f\"‚úì Features scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 4: ESTABLISH BASELINE**\n",
    "\n",
    "**Baseline Strategy**: Always predict the training mean.\n",
    "\n",
    "This tells us how much better our model needs to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE: Always Predict Mean Goals\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Baseline: Always predict training mean\n",
    "baseline_home = y_train_home.mean()\n",
    "baseline_away = y_train_away.mean()\n",
    "\n",
    "# Calculate baseline errors\n",
    "baseline_home_predictions = np.full(len(y_val_home), baseline_home)\n",
    "baseline_away_predictions = np.full(len(y_val_away), baseline_away)\n",
    "\n",
    "baseline_mae_home = mean_absolute_error(y_val_home, baseline_home_predictions)\n",
    "baseline_mae_away = mean_absolute_error(y_val_away, baseline_away_predictions)\n",
    "\n",
    "print(f\"\\nBaseline Strategy: Always predict mean\")\n",
    "print(f\"  Home goals: {baseline_home:.2f}\")\n",
    "print(f\"  Away goals: {baseline_away:.2f}\")\n",
    "\n",
    "print(f\"\\nBaseline MAE:\")\n",
    "print(f\"  Home: {baseline_mae_home:.2f} goals\")\n",
    "print(f\"  Away: {baseline_mae_away:.2f} goals\")\n",
    "\n",
    "print(\"\\nüí° TEACHING POINT:\")\n",
    "print(\"MAE = Mean Absolute Error\")\n",
    "print(\"  Average difference between prediction and actual goals\")\n",
    "print(\"  Lower is better (0 = perfect predictions)\")\n",
    "print(f\"  Baseline MAE ‚âà 1.0 means we're off by ~1 goal per match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 5: TRAIN HOME GOALS REGRESSOR**\n",
    "\n",
    "First model: Predict how many goals the **home team** will score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING MODEL #1: Home Goals Regressor\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize regressor\n",
    "rf_home_goals = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nüå≥ Training Random Forest Regressor for Home Goals...\")\n",
    "print(\"(This may take 30-60 seconds)\")\n",
    "rf_home_goals.fit(X_train_scaled, y_train_home)\n",
    "\n",
    "# Predict\n",
    "y_val_pred_home = rf_home_goals.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate\n",
    "mae_home = mean_absolute_error(y_val_home, y_val_pred_home)\n",
    "rmse_home = np.sqrt(mean_squared_error(y_val_home, y_val_pred_home))\n",
    "r2_home = r2_score(y_val_home, y_val_pred_home)\n",
    "\n",
    "print(f\"\\n‚úì Model trained!\")\n",
    "print(f\"\\nüìä Home Goals Performance:\")\n",
    "print(f\"  MAE:  {mae_home:.3f} goals (avg error)\")\n",
    "print(f\"  RMSE: {rmse_home:.3f} goals (penalizes large errors)\")\n",
    "print(f\"  R¬≤:   {r2_home:.3f} (variance explained: {r2_home*100:.1f}%)\")\n",
    "print(f\"\\nüìà Improvement vs Baseline:\")\n",
    "print(f\"  Baseline MAE: {baseline_mae_home:.3f}\")\n",
    "print(f\"  Model MAE:    {mae_home:.3f}\")\n",
    "print(f\"  Improvement:  {(baseline_mae_home - mae_home):.3f} goals ({((baseline_mae_home - mae_home)/baseline_mae_home)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nüí° TEACHING POINT: What is R¬≤?\")\n",
    "print(\"R¬≤ = How much variance in goals does the model explain?\")\n",
    "print(f\"  R¬≤ = {r2_home:.3f} means model explains {r2_home*100:.1f}% of goal variation\")\n",
    "print(\"  R¬≤ = 1.0 ‚Üí Perfect predictions\")\n",
    "print(\"  R¬≤ = 0.0 ‚Üí No better than predicting mean\")\n",
    "print(f\"  R¬≤ = negative ‚Üí Worse than baseline (rare)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 6: TRAIN AWAY GOALS REGRESSOR**\n",
    "\n",
    "Second model: Predict how many goals the **away team** will score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING MODEL #2: Away Goals Regressor\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize regressor\n",
    "rf_away_goals = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nüå≥ Training Random Forest Regressor for Away Goals...\")\n",
    "rf_away_goals.fit(X_train_scaled, y_train_away)\n",
    "\n",
    "# Predict\n",
    "y_val_pred_away = rf_away_goals.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate\n",
    "mae_away = mean_absolute_error(y_val_away, y_val_pred_away)\n",
    "rmse_away = np.sqrt(mean_squared_error(y_val_away, y_val_pred_away))\n",
    "r2_away = r2_score(y_val_away, y_val_pred_away)\n",
    "\n",
    "print(f\"\\n‚úì Model trained!\")\n",
    "print(f\"\\nüìä Away Goals Performance:\")\n",
    "print(f\"  MAE:  {mae_away:.3f} goals\")\n",
    "print(f\"  RMSE: {rmse_away:.3f} goals\")\n",
    "print(f\"  R¬≤:   {r2_away:.3f} ({r2_away*100:.1f}% variance explained)\")\n",
    "print(f\"\\nüìà Improvement vs Baseline:\")\n",
    "print(f\"  Baseline MAE: {baseline_mae_away:.3f}\")\n",
    "print(f\"  Model MAE:    {mae_away:.3f}\")\n",
    "print(f\"  Improvement:  {(baseline_mae_away - mae_away):.3f} goals ({((baseline_mae_away - mae_away)/baseline_mae_away)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 7: VISUALIZE PREDICTIONS VS ACTUAL**\n",
    "\n",
    "How well do our predictions match reality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Home goals: Predicted vs Actual\n",
    "axes[0].scatter(y_val_home, y_val_pred_home, alpha=0.5, s=50)\n",
    "axes[0].plot([0, 6], [0, 6], 'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual Home Goals', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Predicted Home Goals', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title(f'Home Goals: Predicted vs Actual\\nMAE: {mae_home:.3f}, R¬≤: {r2_home:.3f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Away goals: Predicted vs Actual\n",
    "axes[1].scatter(y_val_away, y_val_pred_away, alpha=0.5, s=50, color='orange')\n",
    "axes[1].plot([0, 6], [0, 6], 'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[1].set_xlabel('Actual Away Goals', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Predicted Away Goals', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title(f'Away Goals: Predicted vs Actual\\nMAE: {mae_away:.3f}, R¬≤: {r2_away:.3f}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'predictions_vs_actual.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° READING THE PLOT:\")\n",
    "print(\"Points on red line = Perfect predictions\")\n",
    "print(\"Points above line = Model over-predicted goals\")\n",
    "print(\"Points below line = Model under-predicted goals\")\n",
    "print(\"Spread = How much error there is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 8: PREDICTION ERROR ANALYSIS**\n",
    "\n",
    "Understanding where the model makes mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTION ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate errors\n",
    "home_errors = y_val_pred_home - y_val_home\n",
    "away_errors = y_val_pred_away - y_val_away\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Home goals error distribution\n",
    "axes[0].hist(home_errors, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero error')\n",
    "axes[0].axvline(home_errors.mean(), color='blue', linestyle='--', linewidth=2, \n",
    "                label=f'Mean error: {home_errors.mean():.3f}')\n",
    "axes[0].set_xlabel('Prediction Error (Predicted - Actual)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Home Goals: Prediction Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Away goals error distribution\n",
    "axes[1].hist(away_errors, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero error')\n",
    "axes[1].axvline(away_errors.mean(), color='blue', linestyle='--', linewidth=2, \n",
    "                label=f'Mean error: {away_errors.mean():.3f}')\n",
    "axes[1].set_xlabel('Prediction Error (Predicted - Actual)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Away Goals: Prediction Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'error_distributions.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nError Statistics:\")\n",
    "print(f\"\\nHome Goals:\")\n",
    "print(f\"  Mean error: {home_errors.mean():.3f} (bias)\")\n",
    "print(f\"  Std error:  {home_errors.std():.3f} (consistency)\")\n",
    "print(f\"  Max over-prediction: {home_errors.max():.2f} goals\")\n",
    "print(f\"  Max under-prediction: {home_errors.min():.2f} goals\")\n",
    "\n",
    "print(f\"\\nAway Goals:\")\n",
    "print(f\"  Mean error: {away_errors.mean():.3f} (bias)\")\n",
    "print(f\"  Std error:  {away_errors.std():.3f} (consistency)\")\n",
    "print(f\"  Max over-prediction: {away_errors.max():.2f} goals\")\n",
    "print(f\"  Max under-prediction: {away_errors.min():.2f} goals\")\n",
    "\n",
    "print(\"\\nüí° TEACHING POINT: Bias vs Variance\")\n",
    "print(\"Mean error close to 0 = UNBIASED (not systematically over/under-predicting)\")\n",
    "print(\"Small std error = LOW VARIANCE (consistent predictions)\")\n",
    "print(\"Good model has both low bias AND low variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 9: DERIVE MATCH OUTCOMES FROM GOALS**\n",
    "\n",
    "Now the key question: Can we derive Win/Draw/Loss from goal predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DERIVING MATCH OUTCOMES FROM GOAL PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def derive_outcome(home_goals, away_goals, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Derive match outcome from goal predictions\n",
    "    \n",
    "    Args:\n",
    "        home_goals: Predicted home goals\n",
    "        away_goals: Predicted away goals\n",
    "        threshold: Goal difference threshold for declaring winner (default 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        'Home Win', 'Away Win', or 'Draw'\n",
    "    \"\"\"\n",
    "    goal_diff = home_goals - away_goals\n",
    "    \n",
    "    if goal_diff > threshold:\n",
    "        return 'Home Win'\n",
    "    elif goal_diff < -threshold:\n",
    "        return 'Away Win'\n",
    "    else:\n",
    "        return 'Draw'\n",
    "\n",
    "# Derive outcomes\n",
    "derived_outcomes = []\n",
    "for home, away in zip(y_val_pred_home, y_val_pred_away):\n",
    "    derived_outcomes.append(derive_outcome(home, away))\n",
    "\n",
    "# Calculate accuracy\n",
    "outcome_accuracy = accuracy_score(y_val_outcome, derived_outcomes)\n",
    "\n",
    "print(f\"\\nüìä OUTCOME ACCURACY (derived from goals):\")\n",
    "print(f\"  Accuracy: {outcome_accuracy:.1%}\")\n",
    "\n",
    "# Compare to Lesson 3 classifier\n",
    "lesson3_metadata = json.load(open('ml_project/outputs/07_model_training/model_metadata.json'))\n",
    "classifier_accuracy = lesson3_metadata['results']['rf_with_tiers']\n",
    "\n",
    "print(f\"\\nüî¨ COMPARISON:\")\n",
    "print(f\"  Lesson 3 Classifier (direct):  {classifier_accuracy:.1%}\")\n",
    "print(f\"  Lesson 3B Goal-based (derived): {outcome_accuracy:.1%}\")\n",
    "print(f\"  Difference: {(outcome_accuracy - classifier_accuracy)*100:+.1f} percentage points\")\n",
    "\n",
    "if outcome_accuracy > classifier_accuracy:\n",
    "    print(\"\\n‚úÖ Goal-based approach is BETTER!\")\n",
    "elif outcome_accuracy > classifier_accuracy - 0.02:\n",
    "    print(\"\\n‚öñÔ∏è  Goal-based approach is COMPARABLE (within 2%)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Goal-based approach is WORSE (but gives more info!)\")\n",
    "\n",
    "print(\"\\nüí° TEACHING POINT:\")\n",
    "print(\"Even if accuracy is slightly lower, goal-based gives you:\")\n",
    "print(\"  ‚úì Score predictions (not just outcome)\")\n",
    "print(\"  ‚úì Goal difference (margin of victory)\")\n",
    "print(\"  ‚úì Over/Under betting opportunities\")\n",
    "print(\"  ‚úì More interpretable (humans think in goals)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 10: CONFUSION MATRIX FOR DERIVED OUTCOMES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFUSION MATRIX: Derived Outcomes\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cm = confusion_matrix(y_val_outcome, derived_outcomes, labels=['Away Win', 'Draw', 'Home Win'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=['Away Win', 'Draw', 'Home Win']\n",
    ")\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d', colorbar=False)\n",
    "plt.title('Confusion Matrix - Goal-Based Predictions\\n(Validation Set)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add per-class accuracy\n",
    "totals_actual = cm.sum(axis=1)\n",
    "for i in range(3):\n",
    "    class_acc = cm[i,i] / totals_actual[i]\n",
    "    ax.text(i, i-0.3, f'{class_acc:.0%}', ha='center', va='center', \n",
    "            color='white', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'confusion_matrix_goal_based.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix\n",
    "totals_predicted = cm.sum(axis=0)\n",
    "print(\"\\n              Predicted\")\n",
    "print(\"              Away   Draw   Home   | Total Actual\")\n",
    "print(f\"Actual  Away   {cm[0,0]:4}   {cm[0,1]:4}   {cm[0,2]:4}  |  {totals_actual[0]:4}\")\n",
    "print(f\"        Draw   {cm[1,0]:4}   {cm[1,1]:4}   {cm[1,2]:4}  |  {totals_actual[1]:4}\")\n",
    "print(f\"        Home   {cm[2,0]:4}   {cm[2,1]:4}   {cm[2,2]:4}  |  {totals_actual[2]:4}\")\n",
    "print(\"        \" + \"-\"*40)\n",
    "print(f\"Total Pred     {totals_predicted[0]:4}   {totals_predicted[1]:4}   {totals_predicted[2]:4}\")\n",
    "\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i, label in enumerate(['Away Win', 'Draw', 'Home Win']):\n",
    "    class_acc = cm[i,i] / totals_actual[i] if totals_actual[i] > 0 else 0\n",
    "    print(f\"  {label:<12}: {cm[i,i]:3}/{totals_actual[i]:3} = {class_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 11: THRESHOLD TUNING**\n",
    "\n",
    "Can we improve accuracy by adjusting the draw threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THRESHOLD TUNING: Finding Optimal Draw Threshold\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Try different thresholds\n",
    "thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "accuracies = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    outcomes = [derive_outcome(h, a, threshold) for h, a in zip(y_val_pred_home, y_val_pred_away)]\n",
    "    acc = accuracy_score(y_val_outcome, outcomes)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Threshold {threshold:.1f}: Accuracy = {acc:.1%}\")\n",
    "\n",
    "# Find best threshold\n",
    "best_idx = np.argmax(accuracies)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_accuracy = accuracies[best_idx]\n",
    "\n",
    "print(f\"\\nüéØ BEST THRESHOLD: {best_threshold:.1f} goals\")\n",
    "print(f\"   Accuracy: {best_accuracy:.1%}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(thresholds, accuracies, marker='o', linewidth=2, markersize=8)\n",
    "ax.axvline(best_threshold, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Best: {best_threshold:.1f} ({best_accuracy:.1%})')\n",
    "ax.set_xlabel('Goal Difference Threshold', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Threshold Tuning: Impact on Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'threshold_tuning.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° TEACHING POINT: What is threshold?\")\n",
    "print(\"Threshold = Goal difference needed to declare a winner\")\n",
    "print(\"  Threshold 0.5: 2.0 vs 1.4 ‚Üí Home Win (diff = 0.6 > 0.5)\")\n",
    "print(\"  Threshold 0.5: 1.8 vs 1.4 ‚Üí Draw (diff = 0.4 < 0.5)\")\n",
    "print(\"  Threshold 0.8: 1.8 vs 1.4 ‚Üí Draw (diff = 0.4 < 0.8)\")\n",
    "print(\"\\nHigher threshold ‚Üí More draws predicted\")\n",
    "print(\"Lower threshold ‚Üí More wins predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 12: FEATURE IMPORTANCE COMPARISON**\n",
    "\n",
    "What features matter for predicting goals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE: What Matters for Goals?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importances\n",
    "home_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_home_goals.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "away_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_away_goals.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualize top 15 for each\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Home goals importance\n",
    "top_home = home_importance.head(15)\n",
    "axes[0].barh(range(len(top_home)), top_home['importance'], color='green', alpha=0.8)\n",
    "axes[0].set_yticks(range(len(top_home)))\n",
    "axes[0].set_yticklabels(top_home['feature'])\n",
    "axes[0].set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Top 15 Features: Home Goals', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Away goals importance\n",
    "top_away = away_importance.head(15)\n",
    "axes[1].barh(range(len(top_away)), top_away['importance'], color='orange', alpha=0.8)\n",
    "axes[1].set_yticks(range(len(top_away)))\n",
    "axes[1].set_yticklabels(top_away['feature'])\n",
    "axes[1].set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Top 15 Features: Away Goals', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'feature_importance_goals.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä TOP 10 FEATURES FOR HOME GOALS:\")\n",
    "for i, (idx, row) in enumerate(home_importance.head(10).iterrows(), 1):\n",
    "    print(f\"{i:2}. {row['feature']:<50} {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\nüìä TOP 10 FEATURES FOR AWAY GOALS:\")\n",
    "for i, (idx, row) in enumerate(away_importance.head(10).iterrows(), 1):\n",
    "    print(f\"{i:2}. {row['feature']:<50} {row['importance']:.4f}\")\n",
    "\n",
    "# Compare to Lesson 3 outcome classifier\n",
    "lesson3_importance = pd.read_csv('ml_project/outputs/07_model_training/feature_importance_full.csv')\n",
    "print(\"\\nüî¨ COMPARISON WITH LESSON 3 (OUTCOME CLASSIFIER):\")\n",
    "print(\"\\nTop 5 for outcome prediction:\")\n",
    "for i, (idx, row) in enumerate(lesson3_importance.head(5).iterrows(), 1):\n",
    "    print(f\"  {i}. {row['feature']}\")\n",
    "\n",
    "print(\"\\nTop 5 for home goals:\")\n",
    "for i, (idx, row) in enumerate(home_importance.head(5).iterrows(), 1):\n",
    "    print(f\"  {i}. {row['feature']}\")\n",
    "\n",
    "print(\"\\nüí° INSIGHT:\")\n",
    "print(\"Different features matter for outcomes vs goals!\")\n",
    "print(\"Outcome model cares about differentials (attack_advantage)\")\n",
    "print(\"Goal models care about absolute attacking strength\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 13: EXAMPLE PREDICTIONS WITH SCORES**\n",
    "\n",
    "Let's see actual score predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE PREDICTIONS: See Scores!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get sample matches\n",
    "sample_indices = data[val_mask].sample(10, random_state=42).index\n",
    "\n",
    "print(\"\\nüéØ 10 Random Validation Matches:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "correct_outcomes = 0\n",
    "total_goal_error = 0\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    match = data.loc[idx]\n",
    "    \n",
    "    # Get features\n",
    "    match_features = X_val_scaled.loc[idx].values.reshape(1, -1)\n",
    "    \n",
    "    # Predict goals\n",
    "    pred_home = rf_home_goals.predict(match_features)[0]\n",
    "    pred_away = rf_away_goals.predict(match_features)[0]\n",
    "    \n",
    "    # Derive outcome\n",
    "    pred_outcome = derive_outcome(pred_home, pred_away, best_threshold)\n",
    "    \n",
    "    # Actual values\n",
    "    actual_home = match['home_goals']\n",
    "    actual_away = match['away_goals']\n",
    "    actual_outcome = match['match_outcome']\n",
    "    \n",
    "    # Check correctness\n",
    "    outcome_correct = \"‚úÖ\" if pred_outcome == actual_outcome else \"‚ùå\"\n",
    "    if pred_outcome == actual_outcome:\n",
    "        correct_outcomes += 1\n",
    "    \n",
    "    # Goal errors\n",
    "    home_error = abs(pred_home - actual_home)\n",
    "    away_error = abs(pred_away - actual_away)\n",
    "    total_goal_error += (home_error + away_error)\n",
    "    \n",
    "    print(f\"\\n{i}. {match['home_team']} vs {match['away_team']}\")\n",
    "    print(f\"   Predicted Score: {pred_home:.1f} - {pred_away:.1f} ({pred_outcome})\")\n",
    "    print(f\"   Actual Score:    {actual_home:.0f} - {actual_away:.0f} ({actual_outcome}) {outcome_correct}\")\n",
    "    print(f\"   Goal Error:      {home_error:.1f} (home), {away_error:.1f} (away)\")\n",
    "\n",
    "print(f\"\\nüìä SAMPLE STATISTICS:\")\n",
    "print(f\"  Outcome accuracy: {correct_outcomes}/10 = {correct_outcomes/10:.0%}\")\n",
    "print(f\"  Avg goal error: {total_goal_error/20:.2f} goals per team\")\n",
    "\n",
    "print(\"\\nüí° WHAT YOU GET WITH GOAL PREDICTIONS:\")\n",
    "print(\"  ‚úì Exact score prediction (2.1 - 1.3)\")\n",
    "print(\"  ‚úì Goal difference (margin of victory)\")\n",
    "print(\"  ‚úì Outcome derived from scores\")\n",
    "print(\"  ‚úì More realistic (how football actually works)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 14: SAVE MODELS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save both goal regressors\n",
    "joblib.dump(rf_home_goals, MODEL_DIR / 'rf_home_goals.pkl')\n",
    "joblib.dump(rf_away_goals, MODEL_DIR / 'rf_away_goals.pkl')\n",
    "\n",
    "print(f\"‚úì Models saved:\")\n",
    "print(f\"  - {MODEL_DIR / 'rf_home_goals.pkl'}\")\n",
    "print(f\"  - {MODEL_DIR / 'rf_away_goals.pkl'}\")\n",
    "\n",
    "# Save metadata\n",
    "model_metadata = {\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'training_samples': len(X_train),\n",
    "    'validation_samples': len(X_val),\n",
    "    'features': len(feature_cols),\n",
    "    'home_goals': {\n",
    "        'mae': float(mae_home),\n",
    "        'rmse': float(rmse_home),\n",
    "        'r2': float(r2_home),\n",
    "        'baseline_mae': float(baseline_mae_home),\n",
    "        'improvement': float(baseline_mae_home - mae_home)\n",
    "    },\n",
    "    'away_goals': {\n",
    "        'mae': float(mae_away),\n",
    "        'rmse': float(rmse_away),\n",
    "        'r2': float(r2_away),\n",
    "        'baseline_mae': float(baseline_mae_away),\n",
    "        'improvement': float(baseline_mae_away - mae_away)\n",
    "    },\n",
    "    'derived_outcome_accuracy': float(outcome_accuracy),\n",
    "    'best_threshold': float(best_threshold),\n",
    "    'comparison': {\n",
    "        'lesson3_classifier': float(classifier_accuracy),\n",
    "        'lesson3b_goal_based': float(outcome_accuracy),\n",
    "        'difference': float(outcome_accuracy - classifier_accuracy)\n",
    "    },\n",
    "    'top_5_features_home': home_importance.head(5)['feature'].tolist(),\n",
    "    'top_5_features_away': away_importance.head(5)['feature'].tolist()\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'goal_model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(f\"  - {OUTPUT_DIR / 'goal_model_metadata.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 15: GENERATE COMPREHENSIVE REPORT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING FINAL REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "report_path = OUTPUT_DIR / 'goal_prediction_report.txt'\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"LESSON 3B: GOAL PREDICTION MODEL REPORT\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"1. DATASET OVERVIEW\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Training samples: {len(X_train)} matches (2020-2024)\\n\")\n",
    "    f.write(f\"Validation samples: {len(X_val)} matches (2024-2025)\\n\")\n",
    "    f.write(f\"Features: {len(feature_cols)}\\n\")\n",
    "    f.write(f\"Targets: home_goals, away_goals\\n\\n\")\n",
    "    \n",
    "    f.write(\"2. GOAL STATISTICS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Home goals - Mean: {data['home_goals'].mean():.2f}, Std: {data['home_goals'].std():.2f}\\n\")\n",
    "    f.write(f\"Away goals - Mean: {data['away_goals'].mean():.2f}, Std: {data['away_goals'].std():.2f}\\n\")\n",
    "    f.write(f\"Home advantage: +{(data['home_goals'].mean() - data['away_goals'].mean()):.2f} goals per match\\n\\n\")\n",
    "    \n",
    "    f.write(\"3. MODEL PERFORMANCE\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Home Goals Regressor:\\n\")\n",
    "    f.write(f\"  MAE:  {mae_home:.3f} goals\\n\")\n",
    "    f.write(f\"  RMSE: {rmse_home:.3f} goals\\n\")\n",
    "    f.write(f\"  R¬≤:   {r2_home:.3f} ({r2_home*100:.1f}% variance explained)\\n\")\n",
    "    f.write(f\"  Improvement vs baseline: {(baseline_mae_home - mae_home):.3f} goals ({((baseline_mae_home - mae_home)/baseline_mae_home)*100:.1f}%)\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Away Goals Regressor:\\n\")\n",
    "    f.write(f\"  MAE:  {mae_away:.3f} goals\\n\")\n",
    "    f.write(f\"  RMSE: {rmse_away:.3f} goals\\n\")\n",
    "    f.write(f\"  R¬≤:   {r2_away:.3f} ({r2_away*100:.1f}% variance explained)\\n\")\n",
    "    f.write(f\"  Improvement vs baseline: {(baseline_mae_away - mae_away):.3f} goals ({((baseline_mae_away - mae_away)/baseline_mae_away)*100:.1f}%)\\n\\n\")\n",
    "    \n",
    "    f.write(\"4. DERIVED OUTCOME ACCURACY\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Outcome accuracy (derived from goals): {outcome_accuracy:.1%}\\n\")\n",
    "    f.write(f\"Best threshold: {best_threshold:.1f} goals\\n\\n\")\n",
    "    \n",
    "    f.write(\"5. COMPARISON WITH LESSON 3 (DIRECT CLASSIFIER)\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Lesson 3 (Direct classifier):  {classifier_accuracy:.1%}\\n\")\n",
    "    f.write(f\"Lesson 3B (Goal-based):         {outcome_accuracy:.1%}\\n\")\n",
    "    f.write(f\"Difference: {(outcome_accuracy - classifier_accuracy)*100:+.1f} percentage points\\n\\n\")\n",
    "    \n",
    "    if outcome_accuracy > classifier_accuracy:\n",
    "        f.write(\"‚úÖ VERDICT: Goal-based approach is BETTER!\\n\")\n",
    "        f.write(\"   Plus: You get score predictions as a bonus!\\n\")\n",
    "    elif outcome_accuracy > classifier_accuracy - 0.02:\n",
    "        f.write(\"‚öñÔ∏è  VERDICT: Goal-based approach is COMPARABLE\\n\")\n",
    "        f.write(\"   Even if slightly lower, you gain score prediction capability!\\n\")\n",
    "    else:\n",
    "        f.write(\"‚ö†Ô∏è  VERDICT: Goal-based approach has lower outcome accuracy\\n\")\n",
    "        f.write(\"   But: You still get valuable score predictions for betting markets\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"6. TOP 10 FEATURES FOR HOME GOALS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    for i, (idx, row) in enumerate(home_importance.head(10).iterrows(), 1):\n",
    "        f.write(f\"{i:2}. {row['feature']:<50} {row['importance']:.4f}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"7. TOP 10 FEATURES FOR AWAY GOALS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    for i, (idx, row) in enumerate(away_importance.head(10).iterrows(), 1):\n",
    "        f.write(f\"{i:2}. {row['feature']:<50} {row['importance']:.4f}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"8. KEY LEARNINGS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(\"‚úì Goal prediction provides richer information than outcome classification\\n\")\n",
    "    f.write(\"‚úì MAE < 1.0 goal means predictions are reasonable\\n\")\n",
    "    f.write(f\"‚úì R¬≤ = {r2_home:.2f} (home) and {r2_away:.2f} (away) shows good explanatory power\\n\")\n",
    "    f.write(\"‚úì Threshold tuning can optimize derived outcome accuracy\\n\")\n",
    "    f.write(\"‚úì Different features matter for goals vs outcomes\\n\\n\")\n",
    "    \n",
    "    f.write(\"9. ADVANTAGES OF GOAL PREDICTION APPROACH\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(\"‚úì Score predictions (e.g., Arsenal 2.1 - 1.3 Chelsea)\\n\")\n",
    "    f.write(\"‚úì Goal difference (margin of victory)\\n\")\n",
    "    f.write(\"‚úì Over/Under betting opportunities (total goals)\\n\")\n",
    "    f.write(\"‚úì Both Teams To Score predictions\\n\")\n",
    "    f.write(\"‚úì More interpretable (humans think in goals)\\n\")\n",
    "    f.write(\"‚úì Can derive outcomes with tunable threshold\\n\\n\")\n",
    "    \n",
    "    f.write(\"10. NEXT STEPS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(\"‚Üí Update prediction script to show score predictions\\n\")\n",
    "    f.write(\"‚Üí Add Poisson distribution modeling for more realistic scores\\n\")\n",
    "    f.write(\"‚Üí Combine both approaches: ensemble classifier + regressor\\n\")\n",
    "    f.write(\"‚Üí Deploy goal prediction to Streamlit dashboard\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"END OF REPORT\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"‚úì Report saved: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **SECTION 16: LEARNING SUMMARY**\n",
    "\n",
    "# üéì **LESSON 3B COMPLETE!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéì LESSON 3B COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìö WHAT YOU LEARNED:\")\n",
    "print(\"‚úì Regression vs Classification (predicting numbers vs categories)\")\n",
    "print(\"‚úì Training goal prediction models (RandomForestRegressor)\")\n",
    "print(\"‚úì Evaluation metrics for regression (MAE, RMSE, R¬≤)\")\n",
    "print(\"‚úì Deriving outcomes from goal predictions\")\n",
    "print(\"‚úì Threshold tuning for optimal performance\")\n",
    "print(\"‚úì Feature importance for goal prediction\")\n",
    "\n",
    "print(\"\\nüìä YOUR RESULTS:\")\n",
    "print(f\"Home Goals MAE: {mae_home:.3f} (avg error)\")\n",
    "print(f\"Away Goals MAE: {mae_away:.3f} (avg error)\")\n",
    "print(f\"Derived Outcome Accuracy: {outcome_accuracy:.1%}\")\n",
    "print(f\"vs Lesson 3 Classifier: {classifier_accuracy:.1%} ({(outcome_accuracy - classifier_accuracy)*100:+.1f} pp)\")\n",
    "\n",
    "print(\"\\nüéØ WHICH IS BETTER?\")\n",
    "if outcome_accuracy > classifier_accuracy:\n",
    "    print(\"‚úÖ Goal-based approach WINS!\")\n",
    "    print(f\"   Better accuracy AND you get score predictions!\")\n",
    "elif outcome_accuracy > classifier_accuracy - 0.02:\n",
    "    print(\"‚öñÔ∏è  Both approaches are COMPARABLE\")\n",
    "    print(f\"   Goal-based gives you score predictions as bonus!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Classifier has better outcome accuracy\")\n",
    "    print(f\"   But goal-based gives you MORE information!\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATION:\")\n",
    "print(\"Use BOTH models for different purposes:\")\n",
    "print(\"  1. Goal prediction ‚Üí Score betting, Over/Under markets\")\n",
    "print(\"  2. Outcome classifier ‚Üí Win/Draw/Loss betting, simpler predictions\")\n",
    "print(\"Or create an ENSEMBLE that combines both!\")\n",
    "\n",
    "print(\"\\nüìÅ FILES CREATED:\")\n",
    "print(f\"  Models:\")\n",
    "print(f\"    - {MODEL_DIR / 'rf_home_goals.pkl'}\")\n",
    "print(f\"    - {MODEL_DIR / 'rf_away_goals.pkl'}\")\n",
    "print(f\"  Outputs:\")\n",
    "print(f\"    - {OUTPUT_DIR / 'goal_distributions.png'}\")\n",
    "print(f\"    - {OUTPUT_DIR / 'predictions_vs_actual.png'}\")\n",
    "print(f\"    - {OUTPUT_DIR / 'error_distributions.png'}\")\n",
    "print(f\"    - {OUTPUT_DIR / 'confusion_matrix_goal_based.png'}\")\n",
    "print(f\"    - {OUTPUT_DIR / 'threshold_tuning.png'}\")\n",
    "print(f\"    - {OUTPUT_DIR / 'feature_importance_goals.png'}\")\n",
    "print(f\"    - {OUTPUT_DIR / 'goal_prediction_report.txt'}\")\n",
    "print(f\"    - {OUTPUT_DIR / 'goal_model_metadata.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ YOU BUILT A GOAL PREDICTION MODEL!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext: Update prediction script to show score predictions!\")\n",
    "print(\"      Run: python ml_project/predict_match.py 'West Ham' 'Brentford'\")\n",
    "print(\"      Output: West Ham 1.8 - 1.2 Brentford (Home Win 62%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
