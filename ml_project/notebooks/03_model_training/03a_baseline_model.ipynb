{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Baseline Model Training\n",
    "\n",
    "## 🎓 LEARNING OBJECTIVES\n",
    "\n",
    "In this notebook, you will:\n",
    "1. Learn how to split data for time-series prediction\n",
    "2. Understand feature scaling and why it matters\n",
    "3. Establish baseline models to beat\n",
    "4. Train your first Random Forest classifier\n",
    "5. **Deeply understand evaluation metrics** (confusion matrix, precision, recall, F1)\n",
    "6. Test whether tier features improve predictions\n",
    "7. Discover which features matter most\n",
    "\n",
    "## 📊 DATASET\n",
    "- **Source**: `ml_project/data/match_features_historical.csv` (from Lesson 2A)\n",
    "- **Size**: 1,900 matches from 2020-2025\n",
    "- **Features**: 111 performance metrics + 3 tier features\n",
    "- **Target**: Match outcome (Home Win / Draw / Away Win)\n",
    "\n",
    "## 🎯 HYPOTHESIS TO TEST\n",
    "**From Lesson 1D**: Different position tiers (Top 4, Mid-Table, Relegation) perform differently.\n",
    "\n",
    "**Question**: Do tier features improve predictions?\n",
    "\n",
    "We'll train TWO models:\n",
    "1. **Model A**: WITHOUT tier features\n",
    "2. **Model B**: WITH tier features\n",
    "\n",
    "Then compare: Does Model B perform better? If yes, hypothesis validated!\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths\n",
    "OUTPUT_DIR = Path('../../outputs/07_model_training')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR = Path('../../models')\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LESSON 3: BASELINE MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n📚 You're about to build your FIRST machine learning model!\")\n",
    "print(\"This notebook will teach you HOW and WHY at every step.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = '../../data/match_features_historical.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"✓ Dataset loaded: {len(data):,} matches\")\n",
    "print(f\"✓ Features: {data.shape[1]} columns\")\n",
    "print(f\"✓ Date range: {data['date'].min()} to {data['date'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Understanding the Target Variable\n",
    "\n",
    "Before building any model, we need to understand **what we're predicting**.\n",
    "\n",
    "Our target variable is `match_outcome` with 3 possible values:\n",
    "- **Home Win**: Home team won\n",
    "- **Draw**: Match ended in a tie\n",
    "- **Away Win**: Away team won\n",
    "\n",
    "This is a **multiclass classification** problem (3 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"UNDERSTANDING OUR TARGET: What are we predicting?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show outcome distribution\n",
    "outcome_counts = data['match_outcome'].value_counts()\n",
    "outcome_pcts = data['match_outcome'].value_counts(normalize=True)\n",
    "\n",
    "print(\"\\nMatch Outcomes Distribution:\")\n",
    "for outcome in ['Home Win', 'Draw', 'Away Win']:\n",
    "    count = outcome_counts.get(outcome, 0)\n",
    "    pct = outcome_pcts.get(outcome, 0)\n",
    "    print(f\"  {outcome:<12}: {count:4} matches ({pct:.1%})\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "bars = ax.bar(outcome_pcts.index, outcome_pcts.values, color=colors, alpha=0.7)\n",
    "ax.set_ylabel('Proportion', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Match Outcome Distribution (1,900 matches)', fontsize=14, fontweight='bold')\n",
    "ax.axhline(0.333, color='black', linestyle='--', alpha=0.3, label='Random guess (33.3%)')\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, pct in zip(bars, outcome_pcts.values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{pct:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'outcome_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 TEACHING POINT:\")\n",
    "print(\"This is an IMBALANCED dataset:\")\n",
    "print(f\"  - Home Win: {outcome_pcts.get('Home Win', 0):.1%} (most common)\")\n",
    "print(f\"  - Draw: {outcome_pcts.get('Draw', 0):.1%} (least common)\")\n",
    "print(f\"  - Away Win: {outcome_pcts.get('Away Win', 0):.1%}\")\n",
    "print(\"\\nThis is NATURAL - home teams really do win more often!\")\n",
    "print(\"Our model needs to learn this pattern, not just guess 'Home Win' always.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Prepare Features & Target\n",
    "\n",
    "In supervised learning, we separate our data into:\n",
    "- **X (features)**: The INPUT data the model sees (statistics, metrics, etc.)\n",
    "- **y (target)**: The OUTPUT we want to predict (match outcome)\n",
    "\n",
    "The model learns patterns: \"When X looks like this, y is usually that.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREPARING DATA: Separating Features (X) from Target (y)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify feature columns (exclude identifiers and targets)\n",
    "exclude_cols = [\n",
    "    'match_id', 'season', 'date', 'gameweek',\n",
    "    'home_team', 'away_team',\n",
    "    'match_outcome', 'home_goals', 'away_goals'\n",
    "]\n",
    "\n",
    "feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "\n",
    "X = data[feature_cols].copy()\n",
    "y = data['match_outcome'].copy()\n",
    "\n",
    "print(f\"\\n✓ Feature matrix X: {X.shape}\")\n",
    "print(f\"  → {X.shape[0]:,} matches × {X.shape[1]} features\")\n",
    "print(f\"\\n✓ Target vector y: {y.shape}\")\n",
    "print(f\"  → {y.shape[0]:,} outcomes to predict\")\n",
    "\n",
    "print(\"\\n💡 TEACHING POINT:\")\n",
    "print(\"In supervised learning:\")\n",
    "print(\"  X = INPUT (what the model sees)\")\n",
    "print(\"  y = OUTPUT (what the model predicts)\")\n",
    "print(f\"\\nOur X has {len(feature_cols)} features like:\")\n",
    "for feat in feature_cols[:5]:\n",
    "    print(f\"  - {feat}\")\n",
    "print(\"  ...\")\n",
    "print(f\"\\nOur y has 3 classes: {sorted(y.unique().tolist())}\")\n",
    "\n",
    "print(f\"\\n📊 Feature types:\")\n",
    "tier_cols = [col for col in feature_cols if 'tier' in col.lower()]\n",
    "stat_cols = [col for col in feature_cols if 'tier' not in col.lower()]\n",
    "print(f\"  - Performance statistics: {len(stat_cols)} features\")\n",
    "print(f\"  - Tier features: {len(tier_cols)} features\")\n",
    "print(f\"    {tier_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Train/Validation Split (Time-Based)\n",
    "\n",
    "### 🚨 CRITICAL CONCEPT: Time-Based Splitting\n",
    "\n",
    "**Wrong way**: Randomly shuffle and split  \n",
    "❌ Problem: Mixes 2020 data with 2024 data. Model sees \"future\" during training!\n",
    "\n",
    "**Right way**: Chronological split  \n",
    "✅ Train on past (2020-2024), validate on recent (2024-2025)\n",
    "\n",
    "This simulates real-world usage: predict future matches using historical patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SPLITTING DATA: Training vs Validation (CHRONOLOGICAL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Time-based split (not random!)\n",
    "train_mask = data['season'].isin(['2020-2021', '2021-2022', '2022-2023', '2023-2024'])\n",
    "val_mask = data['season'] == '2024-2025'\n",
    "\n",
    "X_train = X[train_mask].copy()\n",
    "y_train = y[train_mask].copy()\n",
    "X_val = X[val_mask].copy()\n",
    "y_val = y[val_mask].copy()\n",
    "\n",
    "print(f\"\\n✓ Training set: {len(X_train):,} matches\")\n",
    "print(f\"  Seasons: 2020-2021, 2021-2022, 2022-2023, 2023-2024\")\n",
    "print(f\"\\n✓ Validation set: {len(X_val):,} matches\")\n",
    "print(f\"  Season: 2024-2025\")\n",
    "\n",
    "print(f\"\\nSplit ratio: {len(X_train)/len(X):.0%} train / {len(X_val)/len(X):.0%} validation\")\n",
    "\n",
    "# Check distribution in both sets\n",
    "print(\"\\n📊 Outcome distribution in each set:\")\n",
    "print(\"\\nTraining:\")\n",
    "print(y_train.value_counts(normalize=True).sort_index())\n",
    "print(\"\\nValidation:\")\n",
    "print(y_val.value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\n💡 TEACHING POINT: Why CHRONOLOGICAL split?\")\n",
    "print(\"❌ BAD: Random shuffle\")\n",
    "print(\"   → Mixes 2020 data with 2024 data\")\n",
    "print(\"   → Model sees 'future' during training\")\n",
    "print(\"   → Unrealistic!\")\n",
    "print(\"\\n✓ GOOD: Time-based split\")\n",
    "print(\"   → Train on 2020-2024\")\n",
    "print(\"   → Test on 2024-2025\")\n",
    "print(\"   → Realistic: predict future from past\")\n",
    "print(\"   → This is how it works in production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Feature Scaling (Standardization)\n",
    "\n",
    "### ⚡ CRITICAL: Why Scale Features?\n",
    "\n",
    "Different features have VERY different scales:\n",
    "- `total_pass_distance`: 40,000 (huge!)\n",
    "- `shot_accuracy`: 0.40 (tiny!)\n",
    "\n",
    "Problem: Model thinks distance is 100,000× more important just because the numbers are bigger!\n",
    "\n",
    "Solution: **StandardScaler** transforms all features to:  \n",
    "- Mean ≈ 0  \n",
    "- Standard Deviation ≈ 1\n",
    "\n",
    "Now all features are comparable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE SCALING: Fixing the variance problem\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show the problem BEFORE scaling\n",
    "sample_features = ['home_total_pass_distance', 'home_shot_accuracy', 'home_shots_on_target_per_90']\n",
    "print(\"\\n📊 BEFORE SCALING - Features have very different scales:\")\n",
    "print(X_train[sample_features].describe().loc[['mean', 'std', 'min', 'max']].round(1))\n",
    "\n",
    "print(\"\\n❌ Problem:\")\n",
    "print(\"  total_pass_distance: mean~40,000 (HUGE!)\")\n",
    "print(\"  shot_accuracy: mean~0.4 (tiny!)\")\n",
    "print(\"  → Model thinks distance is 100,000x more important!\")\n",
    "\n",
    "# Initialize and fit scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # ONLY fit on training data!\n",
    "\n",
    "# Transform both sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=feature_cols, index=X_val.index)\n",
    "\n",
    "print(\"\\n📊 AFTER SCALING - All features on same scale:\")\n",
    "print(X_train_scaled[sample_features].describe().loc[['mean', 'std', 'min', 'max']].round(3))\n",
    "\n",
    "print(\"\\n✓ Solution: StandardScaler transforms every feature to:\")\n",
    "print(\"  → Mean ≈ 0\")\n",
    "print(\"  → Standard Deviation ≈ 1\")\n",
    "print(\"  → Now all features are comparable!\")\n",
    "\n",
    "print(\"\\n💡 TEACHING POINT: Why fit ONLY on training?\")\n",
    "print(\"❌ BAD: scaler.fit(X_train + X_val)\")\n",
    "print(\"   → Uses validation data!\")\n",
    "print(\"   → Information leaks from validation into training\")\n",
    "print(\"\\n✓ GOOD: scaler.fit(X_train)\")\n",
    "print(\"   → Only uses training data\")\n",
    "print(\"   → Validation data stays unseen\")\n",
    "print(\"   → Then apply SAME scaling to validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Baseline Models (Benchmarks)\n",
    "\n",
    "### 🎯 KEY CONCEPT: Establish Baselines\n",
    "\n",
    "Before training complex models, establish \"dumb\" baselines:\n",
    "\n",
    "**Baseline 1**: Random guessing (33/33/33)  \n",
    "**Baseline 2**: Always predict \"Home Win\" (most common)  \n",
    "**Baseline 3**: Proportional guessing (43% Home, 23% Draw, 34% Away)\n",
    "\n",
    "### Why baselines matter:\n",
    "- If your ML model gets 40% accuracy → ❌ WORSE than always guessing \"Home Win\" (43%)!\n",
    "- If your ML model gets 58% accuracy → ✅ Beats baseline by 15 points. Model learned something!\n",
    "\n",
    "**Your model MUST beat these baselines to be valuable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"BASELINE MODELS: What do we need to beat?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Baseline 1: Random guessing\n",
    "baseline_random = DummyClassifier(strategy='uniform', random_state=42)\n",
    "baseline_random.fit(X_train_scaled, y_train)\n",
    "y_pred_random = baseline_random.predict(X_val_scaled)\n",
    "acc_random = accuracy_score(y_val, y_pred_random)\n",
    "\n",
    "# Baseline 2: Always predict most frequent class\n",
    "baseline_frequent = DummyClassifier(strategy='most_frequent')\n",
    "baseline_frequent.fit(X_train_scaled, y_train)\n",
    "y_pred_frequent = baseline_frequent.predict(X_val_scaled)\n",
    "acc_frequent = accuracy_score(y_val, y_pred_frequent)\n",
    "\n",
    "# Baseline 3: Proportional guessing\n",
    "baseline_stratified = DummyClassifier(strategy='stratified', random_state=42)\n",
    "baseline_stratified.fit(X_train_scaled, y_train)\n",
    "y_pred_stratified = baseline_stratified.predict(X_val_scaled)\n",
    "acc_stratified = accuracy_score(y_val, y_pred_stratified)\n",
    "\n",
    "print(\"\\n📊 BASELINE RESULTS:\")\n",
    "print(f\"  Random guess (33/33/33): {acc_random:.1%}\")\n",
    "print(f\"  Always 'Home Win': {acc_frequent:.1%}\")\n",
    "print(f\"  Proportional guessing: {acc_stratified:.1%}\")\n",
    "\n",
    "baseline_threshold = max(acc_random, acc_frequent, acc_stratified)\n",
    "print(f\"\\n🎯 TARGET TO BEAT: {baseline_threshold:.1%}\")\n",
    "\n",
    "print(\"\\n💡 TEACHING POINT: Why establish baselines?\")\n",
    "print(\"Baselines = 'Dumb' strategies that require no ML\")\n",
    "print(\"\\nIf your ML model gets 40% accuracy:\")\n",
    "print(\"  ❌ WORSE than always guessing 'Home Win' (43%)\")\n",
    "print(\"  ❌ Model is USELESS!\")\n",
    "print(\"\\nIf your ML model gets 58% accuracy:\")\n",
    "print(\"  ✓ Beats baseline by 15 percentage points\")\n",
    "print(\"  ✓ Model learned something useful!\")\n",
    "print(\"\\nYour model MUST beat these baselines to be valuable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Train Random Forest (WITHOUT Tiers)\n",
    "\n",
    "### 🌳 What is Random Forest?\n",
    "\n",
    "Random Forest = **Ensemble** of 100 decision trees voting\n",
    "\n",
    "Each tree makes a prediction:  \n",
    "- Tree 1: \"Home Win\"  \n",
    "- Tree 2: \"Away Win\"  \n",
    "- Tree 3: \"Home Win\"  \n",
    "- ...  \n",
    "- Tree 100: \"Home Win\"\n",
    "\n",
    "**Final prediction**: Majority vote  \n",
    "→ 65 vote \"Home Win\" → Predict \"Home Win\" (65% confidence)\n",
    "\n",
    "### Why Random Forest?\n",
    "- Handles many features well\n",
    "- Resistant to overfitting\n",
    "- Provides feature importance\n",
    "- Works out-of-the-box with minimal tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING MODEL #1: Random Forest WITHOUT Tier Features\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Remove tier features\n",
    "tier_cols = [col for col in feature_cols if 'tier' in col.lower()]\n",
    "feature_cols_no_tiers = [col for col in feature_cols if col not in tier_cols]\n",
    "\n",
    "X_train_no_tiers = X_train_scaled[feature_cols_no_tiers].copy()\n",
    "X_val_no_tiers = X_val_scaled[feature_cols_no_tiers].copy()\n",
    "\n",
    "print(f\"\\nFeatures used: {len(feature_cols_no_tiers)} (excluded {len(tier_cols)} tier features)\")\n",
    "print(f\"Tier features excluded: {tier_cols}\")\n",
    "\n",
    "# Initialize model\n",
    "rf_no_tiers = RandomForestClassifier(\n",
    "    n_estimators=100,        # 100 decision trees\n",
    "    max_depth=15,            # Max tree depth\n",
    "    min_samples_split=20,    # Min samples to split\n",
    "    min_samples_leaf=10,     # Min samples in leaf\n",
    "    random_state=42,         # Reproducibility\n",
    "    n_jobs=-1,               # Use all CPU cores\n",
    "    class_weight='balanced', # Handle class imbalance\n",
    "    verbose=0                # No progress output\n",
    ")\n",
    "\n",
    "print(\"\\n🌳 Training Random Forest...\")\n",
    "print(\"(This may take 30-60 seconds)\")\n",
    "import time\n",
    "start = time.time()\n",
    "rf_no_tiers.fit(X_train_no_tiers, y_train)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# Predict\n",
    "y_val_pred_no_tiers = rf_no_tiers.predict(X_val_no_tiers)\n",
    "acc_no_tiers = accuracy_score(y_val, y_val_pred_no_tiers)\n",
    "\n",
    "print(f\"\\n✓ Model trained in {elapsed:.1f} seconds!\")\n",
    "print(f\"📊 Validation Accuracy: {acc_no_tiers:.1%}\")\n",
    "print(f\"📈 Improvement vs baseline: {(acc_no_tiers - baseline_threshold)*100:+.1f} percentage points\")\n",
    "\n",
    "if acc_no_tiers > baseline_threshold:\n",
    "    print(\"\\n✅ SUCCESS! Model beats baseline!\")\n",
    "else:\n",
    "    print(\"\\n❌ WARNING: Model doesn't beat baseline. Something's wrong.\")\n",
    "\n",
    "print(\"\\n💡 TEACHING POINT: What is Random Forest?\")\n",
    "print(\"Random Forest = 100 decision trees voting\")\n",
    "print(\"\\nEach tree makes a prediction:\")\n",
    "print(\"  Tree 1: 'Home Win'\")\n",
    "print(\"  Tree 2: 'Away Win'\")\n",
    "print(\"  Tree 3: 'Home Win'\")\n",
    "print(\"  ...\")\n",
    "print(\"  Tree 100: 'Home Win'\")\n",
    "print(\"\\nFinal prediction: Majority vote\")\n",
    "print(\"  65 vote 'Home Win' → Predict 'Home Win' (65% confidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Train Random Forest (WITH Tiers) ⭐\n",
    "\n",
    "### 🧪 HYPOTHESIS TEST\n",
    "\n",
    "Now we'll train a SECOND model that INCLUDES tier features.\n",
    "\n",
    "**Hypothesis**: Different tiers (Top 4, Mid-Table, Relegation) perform differently.  \n",
    "**Test**: Does including tier features improve predictions?\n",
    "\n",
    "If Model WITH tiers > Model WITHOUT tiers → ✅ Hypothesis validated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING MODEL #2: Random Forest WITH Tier Features\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nFeatures used: {len(feature_cols)} (includes {len(tier_cols)} tier features)\")\n",
    "print(f\"Tier features: {tier_cols}\")\n",
    "\n",
    "# Initialize model (same hyperparameters)\n",
    "rf_with_tiers = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n🌳 Training Random Forest...\")\n",
    "start = time.time()\n",
    "rf_with_tiers.fit(X_train_scaled, y_train)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "# Predict\n",
    "y_val_pred_with_tiers = rf_with_tiers.predict(X_val_scaled)\n",
    "acc_with_tiers = accuracy_score(y_val, y_val_pred_with_tiers)\n",
    "\n",
    "print(f\"\\n✓ Model trained in {elapsed:.1f} seconds!\")\n",
    "print(f\"📊 Validation Accuracy: {acc_with_tiers:.1%}\")\n",
    "print(f\"📈 Improvement vs baseline: {(acc_with_tiers - baseline_threshold)*100:+.1f} percentage points\")\n",
    "\n",
    "print(\"\\n🔬 HYPOTHESIS TEST: Do tier features help?\")\n",
    "print(f\"Model WITHOUT tiers: {acc_no_tiers:.1%}\")\n",
    "print(f\"Model WITH tiers: {acc_with_tiers:.1%}\")\n",
    "print(f\"Difference: {(acc_with_tiers - acc_no_tiers)*100:+.1f} percentage points\")\n",
    "\n",
    "if acc_with_tiers > acc_no_tiers + 0.02:\n",
    "    print(\"\\n✅ VALIDATED: Tier features improve predictions!\")\n",
    "    print(\"Your Part 1D insight (different tiers succeed differently) is CORRECT!\")\n",
    "elif acc_with_tiers > acc_no_tiers:\n",
    "    print(\"\\n⚠️ WEAK SUPPORT: Tier features help slightly\")\n",
    "    print(f\"Improvement is small ({(acc_with_tiers - acc_no_tiers)*100:.1f} pp)\")\n",
    "else:\n",
    "    print(\"\\n❌ NOT VALIDATED: Tier features don't help\")\n",
    "    print(\"Model performs same or worse with tier features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Understanding the Confusion Matrix\n",
    "\n",
    "### 🎓 DEEP DIVE: What is a Confusion Matrix?\n",
    "\n",
    "A confusion matrix shows **WHERE your model makes mistakes**.\n",
    "\n",
    "```\n",
    "              Predicted\n",
    "              Away  Draw  Home\n",
    "Actual  Away   XX    YY    ZZ   ← When actual=Away Win, model predicted:\n",
    "        Draw   AA    BB    CC   ← When actual=Draw, model predicted:\n",
    "        Home   DD    EE    FF   ← When actual=Home Win, model predicted:\n",
    "```\n",
    "\n",
    "**Diagonal** (XX, BB, FF) = CORRECT predictions  \n",
    "**Off-diagonal** = ERRORS (confusions)\n",
    "\n",
    "### Example:\n",
    "If `ZZ = 20` (row 1, column 3):  \n",
    "→ 20 times, actual outcome was \"Away Win\" but model predicted \"Home Win\"  \n",
    "→ Model confused away wins for home wins!\n",
    "\n",
    "Let's look at YOUR confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EVALUATION: Understanding the Confusion Matrix\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_val, y_val_pred_with_tiers, labels=['Away Win', 'Draw', 'Home Win'])\n",
    "\n",
    "# Calculate totals for each class\n",
    "totals_actual = cm.sum(axis=1)\n",
    "totals_predicted = cm.sum(axis=0)\n",
    "\n",
    "print(\"\\n📊 CONFUSION MATRIX (Model WITH tiers):\")\n",
    "print(\"\\n              Predicted\")\n",
    "print(\"              Away   Draw   Home   | Total Actual\")\n",
    "print(f\"Actual  Away  {cm[0,0]:4}   {cm[0,1]:4}   {cm[0,2]:4}  |  {totals_actual[0]:4}\")\n",
    "print(f\"        Draw  {cm[1,0]:4}   {cm[1,1]:4}   {cm[1,2]:4}  |  {totals_actual[1]:4}\")\n",
    "print(f\"        Home  {cm[2,0]:4}   {cm[2,1]:4}   {cm[2,2]:4}  |  {totals_actual[2]:4}\")\n",
    "print(\"        \" + \"-\"*40)\n",
    "print(f\"Total Pred    {totals_predicted[0]:4}   {totals_predicted[1]:4}   {totals_predicted[2]:4}\")\n",
    "\n",
    "# Calculate per-class accuracies\n",
    "print(\"\\n📈 Per-Class Accuracy (diagonal / row total):\")\n",
    "class_labels = ['Away Win', 'Draw', 'Home Win']\n",
    "for i, label in enumerate(class_labels):\n",
    "    class_acc = cm[i,i] / totals_actual[i] if totals_actual[i] > 0 else 0\n",
    "    print(f\"  {label:<12}: {cm[i,i]:3}/{totals_actual[i]:3} = {class_acc:.1%}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=['Away Win', 'Draw', 'Home Win']\n",
    ")\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d', colorbar=False)\n",
    "plt.title('Confusion Matrix - Random Forest WITH Tiers\\n(Validation Set)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add accuracy annotations\n",
    "for i in range(3):\n",
    "    class_acc = cm[i,i] / totals_actual[i]\n",
    "    ax.text(i, i-0.3, f'{class_acc:.0%}', ha='center', va='center', \n",
    "            color='white', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'confusion_matrix_with_tiers.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"💡 TEACHING POINT: How to Read This Matrix\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. DIAGONAL = Correct predictions\")\n",
    "print(f\"   - Top-left ({cm[0,0]}): Correctly predicted Away Win\")\n",
    "print(f\"   - Middle ({cm[1,1]}): Correctly predicted Draw\")\n",
    "print(f\"   - Bottom-right ({cm[2,2]}): Correctly predicted Home Win\")\n",
    "print(f\"   - Total correct: {cm[0,0] + cm[1,1] + cm[2,2]} / {totals_actual.sum()} = {acc_with_tiers:.1%}\")\n",
    "\n",
    "print(\"\\n2. OFF-DIAGONAL = Errors (confusions)\")\n",
    "print(f\"   - Row 1, Col 3 ({cm[0,2]}): Away wins predicted as Home wins\")\n",
    "print(f\"   - Row 3, Col 1 ({cm[2,0]}): Home wins predicted as Away wins\")\n",
    "print(f\"   - Draws misclassified: {cm[1,0] + cm[1,2]} times\")\n",
    "\n",
    "print(\"\\n3. INSIGHTS FROM YOUR MATRIX:\")\n",
    "# Find most confused class\n",
    "worst_class_idx = np.argmin([cm[i,i]/totals_actual[i] for i in range(3)])\n",
    "worst_class = class_labels[worst_class_idx]\n",
    "worst_acc = cm[worst_class_idx, worst_class_idx] / totals_actual[worst_class_idx]\n",
    "print(f\"   - Hardest to predict: {worst_class} ({worst_acc:.1%} accuracy)\")\n",
    "\n",
    "best_class_idx = np.argmax([cm[i,i]/totals_actual[i] for i in range(3)])\n",
    "best_class = class_labels[best_class_idx]\n",
    "best_acc = cm[best_class_idx, best_class_idx] / totals_actual[best_class_idx]\n",
    "print(f\"   - Easiest to predict: {best_class} ({best_acc:.1%} accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Precision, Recall, and F1-Score Explained\n",
    "\n",
    "### 🎓 DEEP DIVE: Beyond Accuracy\n",
    "\n",
    "**Accuracy** = Overall correctness  \n",
    "But sometimes we need to know MORE:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. PRECISION: \"When I predict X, how often am I right?\"\n",
    "\n",
    "**Formula**: Precision = True Positives / (True Positives + False Positives)\n",
    "\n",
    "**Example**: Predicting \"Home Win\"  \n",
    "- Model predicted \"Home Win\" 100 times  \n",
    "- 80 were actually Home Wins (✓ correct)  \n",
    "- 20 were actually Draws or Away Wins (✗ wrong)\n",
    "\n",
    "**Precision** = 80 / 100 = **80%**\n",
    "\n",
    "**Meaning**: When model says \"Home Win\", it's right 80% of the time.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. RECALL: \"Of all actual X, how many did I catch?\"\n",
    "\n",
    "**Formula**: Recall = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "**Example**: Predicting \"Home Win\"  \n",
    "- There were 120 actual Home Wins  \n",
    "- Model correctly predicted 80 of them (✓ found)  \n",
    "- Missed 40 (✗ predicted as Draw or Away Win)\n",
    "\n",
    "**Recall** = 80 / 120 = **67%**\n",
    "\n",
    "**Meaning**: Model catches 67% of all Home Wins.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. F1-SCORE: \"Balanced average of precision and recall\"\n",
    "\n",
    "**Formula**: F1 = 2 × (Precision × Recall) / (Precision + Recall)\n",
    "\n",
    "**Why?** Sometimes precision and recall trade off:\n",
    "- High precision, low recall → Model is careful but misses many  \n",
    "- Low precision, high recall → Model catches many but makes many mistakes\n",
    "\n",
    "**F1-score** balances both.\n",
    "\n",
    "---\n",
    "\n",
    "### When to use which metric?\n",
    "\n",
    "**Accuracy**: Overall performance (good for balanced datasets)  \n",
    "**Precision**: When false positives are costly (e.g., spam detection)  \n",
    "**Recall**: When false negatives are costly (e.g., disease diagnosis)  \n",
    "**F1-Score**: When you want balance (most ML tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EVALUATION: Precision, Recall, F1-Score\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use sklearn's classification report\n",
    "report = classification_report(\n",
    "    y_val, \n",
    "    y_val_pred_with_tiers,\n",
    "    labels=['Away Win', 'Draw', 'Home Win'],\n",
    "    target_names=['Away Win', 'Draw', 'Home Win'],\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "print(\"\\n📊 CLASSIFICATION REPORT:\")\n",
    "print(classification_report(\n",
    "    y_val, \n",
    "    y_val_pred_with_tiers,\n",
    "    labels=['Away Win', 'Draw', 'Home Win'],\n",
    "    target_names=['Away Win', 'Draw', 'Home Win']\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"💡 TEACHING: Let's Calculate These BY HAND Using YOUR Data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Let's manually calculate for \"Home Win\" as an example\n",
    "print(\"\\n🏠 Example: 'Home Win' Metrics\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# True Positives: Actual=Home Win, Predicted=Home Win\n",
    "tp_home = cm[2, 2]\n",
    "# False Positives: Actual≠Home Win, Predicted=Home Win\n",
    "fp_home = cm[0, 2] + cm[1, 2]\n",
    "# False Negatives: Actual=Home Win, Predicted≠Home Win\n",
    "fn_home = cm[2, 0] + cm[2, 1]\n",
    "\n",
    "precision_home = tp_home / (tp_home + fp_home) if (tp_home + fp_home) > 0 else 0\n",
    "recall_home = tp_home / (tp_home + fn_home) if (tp_home + fn_home) > 0 else 0\n",
    "f1_home = 2 * (precision_home * recall_home) / (precision_home + recall_home) if (precision_home + recall_home) > 0 else 0\n",
    "\n",
    "print(f\"\\nTrue Positives (TP):  {tp_home}  ← Correctly predicted Home Win\")\n",
    "print(f\"False Positives (FP): {fp_home}  ← Predicted Home Win, but wasn't\")\n",
    "print(f\"False Negatives (FN): {fn_home}  ← Was Home Win, but missed it\")\n",
    "\n",
    "print(f\"\\nPRECISION = TP / (TP + FP)\")\n",
    "print(f\"          = {tp_home} / ({tp_home} + {fp_home})\")\n",
    "print(f\"          = {tp_home} / {tp_home + fp_home}\")\n",
    "print(f\"          = {precision_home:.2f} ({precision_home:.1%})\")\n",
    "print(f\"\\n→ When model predicts 'Home Win', it's right {precision_home:.1%} of the time.\")\n",
    "\n",
    "print(f\"\\nRECALL = TP / (TP + FN)\")\n",
    "print(f\"       = {tp_home} / ({tp_home} + {fn_home})\")\n",
    "print(f\"       = {tp_home} / {tp_home + fn_home}\")\n",
    "print(f\"       = {recall_home:.2f} ({recall_home:.1%})\")\n",
    "print(f\"\\n→ Model catches {recall_home:.1%} of all actual Home Wins.\")\n",
    "\n",
    "print(f\"\\nF1-SCORE = 2 × (Precision × Recall) / (Precision + Recall)\")\n",
    "print(f\"         = 2 × ({precision_home:.2f} × {recall_home:.2f}) / ({precision_home:.2f} + {recall_home:.2f})\")\n",
    "print(f\"         = {f1_home:.2f} ({f1_home:.1%})\")\n",
    "print(f\"\\n→ Balanced score combining precision and recall.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"💡 WHEN TO USE WHICH METRIC?\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. ACCURACY: Overall performance\")\n",
    "print(f\"   Your model: {acc_with_tiers:.1%}\")\n",
    "print(\"   Use when: All classes matter equally\")\n",
    "\n",
    "print(\"\\n2. PRECISION: Avoiding false alarms\")\n",
    "print(f\"   Best class: {class_labels[np.argmax([report[c]['precision'] for c in class_labels])]}\")\n",
    "print(\"   Use when: False positives are costly\")\n",
    "print(\"   Example: Spam detection (don't want good emails in spam)\")\n",
    "\n",
    "print(\"\\n3. RECALL: Catching everything important\")\n",
    "print(f\"   Best class: {class_labels[np.argmax([report[c]['recall'] for c in class_labels])]}\")\n",
    "print(\"   Use when: False negatives are costly\")\n",
    "print(\"   Example: Disease screening (don't want to miss sick patients)\")\n",
    "\n",
    "print(\"\\n4. F1-SCORE: Balanced performance\")\n",
    "print(f\"   Overall: {report['weighted avg']['f1-score']:.1%}\")\n",
    "print(\"   Use when: You want balance between precision and recall\")\n",
    "print(\"   Example: Most ML classification tasks (including this one!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Feature Importance Analysis\n",
    "\n",
    "### 🔍 What Matters Most?\n",
    "\n",
    "Random Forest tells us **how much each feature contributed** to predictions.\n",
    "\n",
    "**Feature Importance** = How much the model relies on each feature\n",
    "\n",
    "Example:  \n",
    "- `shots_on_target_per_90 = 0.15` (15%) → 15% of decisions use this feature  \n",
    "- `some_random_stat = 0.001` (0.1%) → Model barely uses it\n",
    "\n",
    "This helps us:\n",
    "1. Understand what drives predictions\n",
    "2. Remove useless features\n",
    "3. Validate domain knowledge\n",
    "4. Test if tier features matter (our hypothesis!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE: What matters most?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_with_tiers.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Save full list\n",
    "feature_importance.to_csv(OUTPUT_DIR / 'feature_importance_full.csv', index=False)\n",
    "\n",
    "print(\"\\n📊 TOP 20 MOST IMPORTANT FEATURES:\")\n",
    "for i, (idx, row) in enumerate(feature_importance.head(20).iterrows(), 1):\n",
    "    bar = '█' * int(row['importance'] * 200)  # Scale for visibility\n",
    "    tier_marker = \" 🎯\" if 'tier' in row['feature'].lower() else \"\"\n",
    "    print(f\"{i:2}. {row['feature']:<50} {row['importance']:.4f} {bar}{tier_marker}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "top_20 = feature_importance.head(20)\n",
    "colors = ['darkgreen' if 'tier' in feat.lower() else 'steelblue' for feat in top_20['feature']]\n",
    "ax.barh(range(len(top_20)), top_20['importance'], color=colors, alpha=0.8)\n",
    "ax.set_yticks(range(len(top_20)))\n",
    "ax.set_yticklabels(top_20['feature'])\n",
    "ax.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 20 Most Important Features\\n(Green = Tier features)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'feature_importance_top20.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Check tier feature importance\n",
    "tier_importance = feature_importance[feature_importance['feature'].str.contains('tier', case=False)]\n",
    "print(\"\\n🎯 TIER FEATURE IMPORTANCE:\")\n",
    "print(tier_importance.to_string(index=False))\n",
    "print(f\"\\nTotal tier importance: {tier_importance['importance'].sum():.1%}\")\n",
    "if len(tier_importance) > 0:\n",
    "    first_tier_idx = feature_importance.index[feature_importance['feature'].isin(tier_cols)][0]\n",
    "    first_tier_rank = list(feature_importance.index).index(first_tier_idx) + 1\n",
    "    print(f\"Highest ranking tier feature: #{first_tier_rank} ({feature_importance.iloc[first_tier_rank-1]['feature']})\")\n",
    "\n",
    "print(\"\\n💡 TEACHING POINT:\")\n",
    "print(\"Feature importance = How much the model relies on each feature\")\n",
    "print(\"\\nIf shots_on_target_per_90 = 0.15 (15%):\")\n",
    "print(\"  → 15% of the model's decisions use this feature\")\n",
    "print(\"  → One of the most important features\")\n",
    "print(\"\\nIf some_random_stat = 0.001 (0.1%):\")\n",
    "print(\"  → Model barely uses it\")\n",
    "print(\"  → Could remove without hurting performance\")\n",
    "\n",
    "# Check if gold standard features are important\n",
    "gold_features = ['home_shots_on_target_per_90', 'away_shots_on_target_per_90',\n",
    "                'home_touches_att_penalty', 'away_touches_att_penalty']\n",
    "gold_in_top20 = [f for f in gold_features if f in top_20['feature'].values]\n",
    "if gold_in_top20:\n",
    "    print(f\"\\n⭐ VALIDATION: Gold standard features in top 20: {gold_in_top20}\")\n",
    "    print(\"Your Part 1D correlations were RIGHT!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Model Comparison\n",
    "\n",
    "Let's compare ALL our models side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON: Which performs best?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compile results\n",
    "results = {\n",
    "    'Model': [\n",
    "        'Random Guess',\n",
    "        'Always \"Home Win\"',\n",
    "        'Proportional Guess',\n",
    "        'Random Forest (No Tiers)',\n",
    "        'Random Forest (With Tiers)'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        acc_random,\n",
    "        acc_frequent,\n",
    "        acc_stratified,\n",
    "        acc_no_tiers,\n",
    "        acc_with_tiers\n",
    "    ],\n",
    "    'Type': [\n",
    "        'Baseline',\n",
    "        'Baseline',\n",
    "        'Baseline',\n",
    "        'ML Model',\n",
    "        'ML Model'\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n📊 RESULTS SUMMARY:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['gray' if t=='Baseline' else 'darkgreen' if 'With Tiers' in m else 'steelblue' \n",
    "          for m, t in zip(results_df['Model'], results_df['Type'])]\n",
    "bars = ax.barh(results_df['Model'], results_df['Accuracy'], color=colors, alpha=0.8)\n",
    "ax.set_xlabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.axvline(baseline_threshold, color='red', linestyle='--', linewidth=2, label='Baseline Threshold')\n",
    "\n",
    "# Add labels\n",
    "for bar, acc in zip(bars, results_df['Accuracy']):\n",
    "    ax.text(acc + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "            f'{acc:.1%}', va='center', fontweight='bold')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🏆 BEST MODEL: {results_df.iloc[0]['Model']}\")\n",
    "print(f\"📈 Accuracy: {results_df.iloc[0]['Accuracy']:.1%}\")\n",
    "print(f\"🎯 Beat baseline by: {(results_df.iloc[0]['Accuracy'] - baseline_threshold)*100:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13: Example Predictions\n",
    "\n",
    "Let's see the model in action! We'll look at 10 random validation matches and see:\n",
    "- What the model predicted\n",
    "- What actually happened\n",
    "- How confident the model was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXAMPLE PREDICTIONS: See the model in action\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get sample matches\n",
    "val_data = data[val_mask].copy()\n",
    "sample_indices = val_data.sample(10, random_state=42).index\n",
    "\n",
    "print(\"\\n🎯 10 Random Validation Matches:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    match = data.loc[idx]\n",
    "    \n",
    "    # Get features\n",
    "    match_features = X_val_scaled.loc[idx].values.reshape(1, -1)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = rf_with_tiers.predict(match_features)[0]\n",
    "    probabilities = rf_with_tiers.predict_proba(match_features)[0]\n",
    "    \n",
    "    # Map to classes\n",
    "    classes = rf_with_tiers.classes_\n",
    "    prob_dict = {cls: prob for cls, prob in zip(classes, probabilities)}\n",
    "    \n",
    "    actual = match['match_outcome']\n",
    "    correct = \"✅\" if prediction == actual else \"❌\"\n",
    "    \n",
    "    print(f\"\\n{i}. {match['home_team']} vs {match['away_team']}\")\n",
    "    print(f\"   Date: {match['date']} | Gameweek: {int(match['gameweek'])}\")\n",
    "    print(f\"   Actual: {actual} | Predicted: {prediction} {correct}\")\n",
    "    print(f\"   Confidence: \", end=\"\")\n",
    "    for cls in ['Home Win', 'Draw', 'Away Win']:\n",
    "        if cls in prob_dict:\n",
    "            marker = \"←\" if cls == prediction else \"\"\n",
    "            print(f\"{cls}: {prob_dict[cls]:.0%}{marker}  \", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n💡 TEACHING POINT:\")\n",
    "print(\"predict_proba() gives CONFIDENCE scores\")\n",
    "print(\"\\nExample: Arsenal vs Chelsea\")\n",
    "print(\"  Home Win: 62%  ← Model is 62% confident\")\n",
    "print(\"  Draw: 23%\")\n",
    "print(\"  Away Win: 15%\")\n",
    "print(\"\\nHigher confidence = Model is more sure\")\n",
    "print(\"Lower confidence = Uncertain match\")\n",
    "print(\"\\nEven when wrong, looking at confidence helps:\")\n",
    "print(\"  - High confidence + wrong = Model has wrong assumptions\")\n",
    "print(\"  - Low confidence + wrong = Genuinely unpredictable match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14: Save Models\n",
    "\n",
    "Let's save our trained models so we can use them later without retraining!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save both models\n",
    "joblib.dump(rf_no_tiers, MODEL_DIR / 'rf_no_tiers.pkl')\n",
    "joblib.dump(rf_with_tiers, MODEL_DIR / 'rf_with_tiers.pkl')\n",
    "joblib.dump(scaler, MODEL_DIR / 'feature_scaler.pkl')\n",
    "\n",
    "print(\"\\n✓ Models saved:\")\n",
    "print(f\"  - {MODEL_DIR / 'rf_no_tiers.pkl'}\")\n",
    "print(f\"  - {MODEL_DIR / 'rf_with_tiers.pkl'}\")\n",
    "print(f\"  - {MODEL_DIR / 'feature_scaler.pkl'}\")\n",
    "\n",
    "# Save metadata\n",
    "model_metadata = {\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'training_samples': len(X_train),\n",
    "    'validation_samples': len(X_val),\n",
    "    'features_total': len(feature_cols),\n",
    "    'features_no_tiers': len(feature_cols_no_tiers),\n",
    "    'tier_features': tier_cols,\n",
    "    'hyperparameters': {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 15,\n",
    "        'min_samples_split': 20,\n",
    "        'min_samples_leaf': 10,\n",
    "        'class_weight': 'balanced'\n",
    "    },\n",
    "    'results': {\n",
    "        'baseline_random': float(acc_random),\n",
    "        'baseline_frequent': float(acc_frequent),\n",
    "        'rf_no_tiers': float(acc_no_tiers),\n",
    "        'rf_with_tiers': float(acc_with_tiers),\n",
    "        'tier_improvement': float(acc_with_tiers - acc_no_tiers),\n",
    "        'baseline_improvement': float(acc_with_tiers - baseline_threshold)\n",
    "    },\n",
    "    'top_5_features': feature_importance.head(5)['feature'].tolist(),\n",
    "    'tier_feature_total_importance': float(tier_importance['importance'].sum())\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(f\"  - {OUTPUT_DIR / 'model_metadata.json'}\")\n",
    "\n",
    "print(\"\\n💡 To load these models later:\")\n",
    "print(\"```python\")\n",
    "print(\"import joblib\")\n",
    "print(\"model = joblib.load('path/to/rf_with_tiers.pkl')\")\n",
    "print(\"scaler = joblib.load('path/to/feature_scaler.pkl')\")\n",
    "print(\"predictions = model.predict(scaler.transform(new_data))\")\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 15: Generate Comprehensive Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GENERATING FINAL REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "report_path = OUTPUT_DIR / 'training_report.txt'\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"LESSON 3: BASELINE MODEL TRAINING REPORT\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"1. DATASET OVERVIEW\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Training samples: {len(X_train):,} matches (2020-2024)\\n\")\n",
    "    f.write(f\"Validation samples: {len(X_val):,} matches (2024-2025)\\n\")\n",
    "    f.write(f\"Total features: {len(feature_cols)}\\n\")\n",
    "    f.write(f\"Target classes: Away Win / Draw / Home Win\\n\\n\")\n",
    "    \n",
    "    f.write(\"2. BASELINE PERFORMANCE\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Random guess: {acc_random:.1%}\\n\")\n",
    "    f.write(f\"Always 'Home Win': {acc_frequent:.1%}\\n\")\n",
    "    f.write(f\"Proportional guessing: {acc_stratified:.1%}\\n\")\n",
    "    f.write(f\"Threshold to beat: {baseline_threshold:.1%}\\n\\n\")\n",
    "    \n",
    "    f.write(\"3. MODEL PERFORMANCE\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Random Forest (no tiers): {acc_no_tiers:.1%}\\n\")\n",
    "    f.write(f\"Random Forest (with tiers): {acc_with_tiers:.1%}\\n\\n\")\n",
    "    f.write(f\"Improvement from tiers: {(acc_with_tiers - acc_no_tiers)*100:+.1f} pp\\n\")\n",
    "    f.write(f\"Improvement vs baseline: {(acc_with_tiers - baseline_threshold)*100:+.1f} pp\\n\\n\")\n",
    "    \n",
    "    f.write(\"4. HYPOTHESIS TEST: Do Tier Features Help?\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    if acc_with_tiers > acc_no_tiers + 0.02:\n",
    "        f.write(\"✅ VALIDATED: Tier features significantly improve predictions!\\n\")\n",
    "        f.write(f\"   Model WITH tiers beats model WITHOUT tiers by {(acc_with_tiers - acc_no_tiers)*100:.1f} pp\\n\")\n",
    "        f.write(\"   Your Part 1D insight is CORRECT!\\n\")\n",
    "        f.write(\"   Different tiers DO succeed differently, and the model learned this.\\n\")\n",
    "    elif acc_with_tiers > acc_no_tiers:\n",
    "        f.write(\"⚠️ WEAK SUPPORT: Tier features help slightly\\n\")\n",
    "        f.write(f\"   Improvement is small ({(acc_with_tiers - acc_no_tiers)*100:.1f} pp)\\n\")\n",
    "        f.write(\"   Hypothesis has some validity but effect is weak.\\n\")\n",
    "    else:\n",
    "        f.write(\"❌ NOT VALIDATED: Tier features don't help\\n\")\n",
    "        f.write(\"   Model performs same or worse with tier features.\\n\")\n",
    "        f.write(\"   Hypothesis needs revision.\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"5. CONFUSION MATRIX ANALYSIS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(\"Confusion Matrix (Model WITH tiers):\\n\\n\")\n",
    "    f.write(\"              Predicted\\n\")\n",
    "    f.write(\"              Away   Draw   Home   | Total Actual\\n\")\n",
    "    f.write(f\"Actual  Away   {cm[0,0]:4}   {cm[0,1]:4}   {cm[0,2]:4}  |  {totals_actual[0]:4}\\n\")\n",
    "    f.write(f\"        Draw   {cm[1,0]:4}   {cm[1,1]:4}   {cm[1,2]:4}  |  {totals_actual[1]:4}\\n\")\n",
    "    f.write(f\"        Home   {cm[2,0]:4}   {cm[2,1]:4}   {cm[2,2]:4}  |  {totals_actual[2]:4}\\n\")\n",
    "    f.write(\"        \" + \"-\"*40 + \"\\n\")\n",
    "    f.write(f\"Total Pred     {totals_predicted[0]:4}   {totals_predicted[1]:4}   {totals_predicted[2]:4}\\n\\n\")\n",
    "    \n",
    "    # Calculate per-class accuracies\n",
    "    f.write(\"Per-Class Accuracy:\\n\")\n",
    "    for i, label in enumerate(class_labels):\n",
    "        class_acc = cm[i,i] / totals_actual[i] if totals_actual[i] > 0 else 0\n",
    "        f.write(f\"  {label:<12}: {cm[i,i]:3}/{totals_actual[i]:3} = {class_acc:.1%}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"6. CLASSIFICATION METRICS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    for label in class_labels:\n",
    "        f.write(f\"\\n{label}:\\n\")\n",
    "        f.write(f\"  Precision: {report[label]['precision']:.2f}\\n\")\n",
    "        f.write(f\"  Recall:    {report[label]['recall']:.2f}\\n\")\n",
    "        f.write(f\"  F1-Score:  {report[label]['f1-score']:.2f}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"7. TOP 10 MOST IMPORTANT FEATURES\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    for i, (idx, row) in enumerate(feature_importance.head(10).iterrows(), 1):\n",
    "        tier_marker = \" 🎯\" if 'tier' in row['feature'].lower() else \"\"\n",
    "        f.write(f\"{i:2}. {row['feature']:<50} {row['importance']:.4f}{tier_marker}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"8. TIER FEATURE ANALYSIS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Total tier importance: {tier_importance['importance'].sum():.1%}\\n\")\n",
    "    f.write(f\"Number of tier features: {len(tier_cols)}\\n\")\n",
    "    f.write(\"Individual tier feature importance:\\n\")\n",
    "    for idx, row in tier_importance.iterrows():\n",
    "        rank = list(feature_importance.index).index(idx) + 1\n",
    "        f.write(f\"  {row['feature']:<35} Rank #{rank:3}  Importance: {row['importance']:.4f}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"9. KEY LEARNINGS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"✓ Model significantly beats baseline ({(acc_with_tiers - baseline_threshold)*100:+.1f} pp)\\n\")\n",
    "    f.write(\"✓ ML is working - the model learned useful patterns!\\n\")\n",
    "    f.write(\"✓ Feature scaling successfully fixed variance issues\\n\")\n",
    "    f.write(f\"✓ Random Forest handles {len(feature_cols)} features effectively\\n\")\n",
    "    \n",
    "    # Check if gold standard features are important\n",
    "    if gold_in_top20:\n",
    "        f.write(\"✓ Gold standard features (Part 1D) rank highly - correlation validated!\\n\")\n",
    "    \n",
    "    tier_in_top20 = any('tier' in feat.lower() for feat in feature_importance.head(20)['feature'].values)\n",
    "    if tier_in_top20:\n",
    "        f.write(\"✓ Tier features in top 20 - domain knowledge adds value!\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"10. NEXT STEPS\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(\"→ Lesson 3B: Hyperparameter tuning (optimize performance)\\n\")\n",
    "    f.write(\"→ Lesson 4: Add current season features (fine-tuning on 2025-2026)\\n\")\n",
    "    f.write(\"→ Phase 2: Deploy to Streamlit dashboard (real predictions)\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"END OF REPORT\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"\\n✓ Report saved: {report_path}\")\n",
    "print(\"\\nYou can read this report anytime to remember your results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 16: Learning Summary\n",
    "\n",
    "# 🎉 LESSON 3 COMPLETE!\n",
    "\n",
    "## 📚 WHAT YOU LEARNED:\n",
    "\n",
    "### 1. Data Preparation\n",
    "- ✅ **Train/test splitting** (chronological, not random)\n",
    "- ✅ **Feature scaling** (StandardScaler - why and how)\n",
    "- ✅ **Separating X and y** (features vs target)\n",
    "\n",
    "### 2. Model Training\n",
    "- ✅ **Baseline models** (establishing benchmarks)\n",
    "- ✅ **Random Forest classifier** (how ensemble learning works)\n",
    "- ✅ **Hyperparameters** (n_estimators, max_depth, etc.)\n",
    "\n",
    "### 3. Model Evaluation (DEEP UNDERSTANDING)\n",
    "- ✅ **Confusion Matrix** - where the model makes mistakes\n",
    "- ✅ **Accuracy** - overall correctness\n",
    "- ✅ **Precision** - \"When I predict X, how often am I right?\"\n",
    "- ✅ **Recall** - \"Of all actual X, how many did I catch?\"\n",
    "- ✅ **F1-Score** - balanced precision and recall\n",
    "\n",
    "### 4. Feature Analysis\n",
    "- ✅ **Feature importance** (what drives predictions)\n",
    "- ✅ **Domain validation** (tier features, gold standard metrics)\n",
    "\n",
    "### 5. Hypothesis Testing\n",
    "- ✅ **Scientific approach** (WITH vs WITHOUT tier features)\n",
    "- ✅ **Quantitative validation** (measure improvement)\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 YOUR ACHIEVEMENTS:\n",
    "\n",
    "You built your FIRST machine learning model and learned:\n",
    "- WHY each step matters\n",
    "- HOW to interpret results\n",
    "- WHEN to use which metric\n",
    "- WHAT makes a good model\n",
    "\n",
    "You didn't just run code - you UNDERSTOOD the concepts!\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 NEXT STEPS:\n",
    "\n",
    "**Lesson 3B**: Hyperparameter tuning  \n",
    "→ Optimize model performance through systematic parameter search\n",
    "\n",
    "**Lesson 4**: Current season features  \n",
    "→ Add 2025-2026 data and fine-tune for latest patterns\n",
    "\n",
    "**Phase 2**: Production deployment  \n",
    "→ Build Streamlit dashboard with real predictions\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 HOMEWORK (Optional):\n",
    "\n",
    "1. Look at YOUR confusion matrix. Which class is hardest to predict? Why?\n",
    "2. Look at YOUR feature importance. Do the top features make sense?\n",
    "3. Look at YOUR example predictions. When is the model confident? When uncertain?\n",
    "\n",
    "**The more you engage with YOUR specific results, the deeper your understanding!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎓 LESSON 3 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📚 WHAT YOU LEARNED:\")\n",
    "print(\"✓ Train/test splitting (chronological, not random)\")\n",
    "print(\"✓ Feature scaling (StandardScaler)\")\n",
    "print(\"✓ Baseline models (benchmarks)\")\n",
    "print(\"✓ Random Forest classifier (how it works)\")\n",
    "print(\"✓ Model evaluation (accuracy, confusion matrix)\")\n",
    "print(\"✓ Precision, Recall, F1 (DEEP understanding)\")\n",
    "print(\"✓ Feature importance (what matters most)\")\n",
    "print(\"✓ Hypothesis testing (do tier features help?)\")\n",
    "\n",
    "print(\"\\n📊 YOUR RESULTS:\")\n",
    "print(f\"Baseline: {baseline_threshold:.1%}\")\n",
    "print(f\"Your Model: {acc_with_tiers:.1%}\")\n",
    "print(f\"Improvement: {(acc_with_tiers - baseline_threshold)*100:+.1f} percentage points\")\n",
    "\n",
    "print(\"\\n🎯 HYPOTHESIS TEST:\")\n",
    "if acc_with_tiers > acc_no_tiers + 0.02:\n",
    "    print(\"✅ Tier features significantly help!\")\n",
    "    print(f\"   +{(acc_with_tiers - acc_no_tiers)*100:.1f} pp improvement\")\n",
    "elif acc_with_tiers > acc_no_tiers:\n",
    "    print(\"⚠️ Tier features help slightly\")\n",
    "    print(f\"   +{(acc_with_tiers - acc_no_tiers)*100:.1f} pp improvement\")\n",
    "else:\n",
    "    print(\"❌ Tier features don't help\")\n",
    "\n",
    "print(\"\\n📁 FILES CREATED:\")\n",
    "print(f\"  Models:\")\n",
    "print(f\"    - {MODEL_DIR / 'rf_no_tiers.pkl'}\")\n",
    "print(f\"    - {MODEL_DIR / 'rf_with_tiers.pkl'}\")\n",
    "print(f\"    - {MODEL_DIR / 'feature_scaler.pkl'}\")\n",
    "print(f\"  Outputs:\")\n",
    "print(f\"    - {OUTPUT_DIR / 'confusion_matrix_with_tiers.png'}\")\n",
    "print(f\"    - {OUTPUT_DIR / 'feature_importance_top20.png'}\")\n",
    "print(f\"    - {OUTPUT_DIR / 'model_comparison.png'}\")\n",
    "print(f\"    - {OUTPUT_DIR / 'training_report.txt'}\")\n",
    "print(f\"    - {OUTPUT_DIR / 'model_metadata.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 YOU BUILT YOUR FIRST ML MODEL!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYou didn't just run code - you UNDERSTOOD:\")\n",
    "print(\"  - WHY each step matters\")\n",
    "print(\"  - HOW to interpret results\")\n",
    "print(\"  - WHEN to use which metric\")\n",
    "print(\"  - WHAT makes a good model\")\n",
    "print(\"\\n🚀 Ready for the next lesson!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
